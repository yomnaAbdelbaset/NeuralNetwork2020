{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Yomna_codes.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/sbH3mESsX7hHiVlFeVir",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yomnaAbdelbaset/NeuralNetwork2020/blob/master/Yomna_codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJS-SkrKbblA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "N, D, H = 64, 1000, 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzWuIjt5daCZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code #1\n",
        "x=tf.placeholder(tf.float32, shape=(N, D))\n",
        "y=tf.placeholder(tf.float32, shape=(N, D))\n",
        "w1=tf.placeholder(tf.float32, shape=(D, H))\n",
        "w2=tf.placeholder(tf.float32, shape=(H, D))\n",
        "\n",
        "h = tf.maximum(tf.matmul(x, w1), 0)\n",
        "y_pred = tf.matmul(h, w2)\n",
        "diff = y_pred - y\n",
        "loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))\n",
        "\n",
        "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  values = {x : np.random.rand(N, D),\n",
        "            w1 : np.random.randn(D, H),\n",
        "            w2 : np.random.randn(H, D),\n",
        "            y : np.random.randn(N, D),}\n",
        "  out = sess.run([loss, grad_w1, grad_w2],\n",
        "                 feed_dict=values)\n",
        "  loss_val, grad_w1_val, grad_w2_val = out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Ab7qAeftfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code #2\n",
        "x=tf.placeholder(tf.float32, shape=(N, D))\n",
        "y=tf.placeholder(tf.float32, shape=(N, D))\n",
        "w1=tf.placeholder(tf.float32, shape=(D, H))\n",
        "w2=tf.placeholder(tf.float32, shape=(H, D))\n",
        "\n",
        "h = tf.maximum(tf.matmul(x, w1), 0)\n",
        "y_pred = tf.matmul(h, w2)\n",
        "diff = y_pred - y\n",
        "loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))\n",
        "\n",
        "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  values = {x : np.random.rand(N, D),\n",
        "            w1 : np.random.randn(D, H),\n",
        "            w2 : np.random.randn(H, D),\n",
        "            y : np.random.randn(N, D),}\n",
        "  learning_rate = 1e-5\n",
        "  for t in range(50):\n",
        "    out = sess.run([loss, grad_w1, grad_w2],\n",
        "                   feed_dict=values)\n",
        "    loss_val, grad_w1_val, grad_w2_val =out\n",
        "    values[w1] -= learning_rate * grad_w1_val\n",
        "    values[w2] -= learning_rate * grad_w2_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdflYru5g82w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code #3\n",
        "x=tf.placeholder(tf.float32, shape=(N, D))\n",
        "y=tf.placeholder(tf.float32, shape=(N, D))\n",
        "w1=tf.Variable(tf.random_normal((D, H)))\n",
        "w2=tf.Variable(tf.random_normal((H, D)))\n",
        "\n",
        "h = tf.maximum(tf.matmul(x, w1), 0)\n",
        "y_pred = tf.matmul(h, w2)\n",
        "diff = y_pred - y\n",
        "loss = tf.reduce_mean(tf.reduce_sum(diff ** 2, axis = 1))\n",
        "\n",
        "grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])\n",
        "\n",
        "learning_rate = 1e-5\n",
        "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
        "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
        "updates = tf.group(new_w1, new_w2)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  values = {x: np.random.randn(N, D),\n",
        "            y: np.random.randn(N, D),}\n",
        "  losses= []\n",
        "  for t in range(50):\n",
        "    loss_val, _ = sess.run([loss, updates], feed_dict=values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvUs9OXsiTNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code #4\n",
        "x=tf.placeholder(tf.float32, shape=(N, D))\n",
        "y=tf.placeholder(tf.float32, shape=(N, D))\n",
        "w1=tf.Variable(tf.random_normal((D, H)))\n",
        "w2=tf.Variable(tf.random_normal((H, D)))\n",
        "\n",
        "h = tf.maximum(tf.matmul(x, w1), 0)\n",
        "y_pred = tf.matmul(h, w2)\n",
        "loss = tf.losses.mean_pairwise_squared_error(y_pred, y)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
        "updates = optimizer.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  values = {x: np.random.randn(N, D),\n",
        "            y: np.random.randn(N, D),}\n",
        "  for t in range(50):\n",
        "    loss_val, _ = sess.run([loss, updates], feed_dict=values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjtjhi7RmcYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code #5\n",
        "x=tf.placeholder(tf.float32, shape=(N, D))\n",
        "y=tf.placeholder(tf.float32, shape=(N, D))\n",
        "\n",
        "init = tf.variance_scaling_initializer(2.0)\n",
        "h = tf.layers.dense(inputs = x, units = H,\n",
        "                    activation = tf.nn.relu, kernel_initializer = init)\n",
        "y_pred = tf.layers.dense(inputs=h, units=D, kernel_initializer=init)\n",
        "\n",
        "loss=tf.losses.mean_squared_error(y_pred, y)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(1e0)\n",
        "updates = optimizer.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  values = {x: np.random.randn(N, D),\n",
        "            y: np.random.randn(N, D),}\n",
        "  for t in range(50):\n",
        "    loss_val, _ = sess.run([loss, updates], feed_dict=values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8DcR5h1noc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code #6\n",
        "x=tf.placeholder(tf.float32, shape=(N, D))\n",
        "y=tf.placeholder(tf.float32, shape=(N, D))\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(H, input_shape=(D,), activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(D))\n",
        "y_pred = model(x)\n",
        "loss = tf.losses.mean_squared_error(y_pred, y)\n",
        "\n",
        "optimizer = tf.train.GradientDescentOptimizer(1e0)\n",
        "updates = optimizer.minimize(loss)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  values = {x: np.random.randn(N, D),\n",
        "            y: np.random.randn(N, D),}\n",
        "  for t in range(50):\n",
        "    loss_val, _ = sess.run([loss, updates], feed_dict=values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYrRn-hIoD0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b9ae396c-b140-4ce0-e780-3c640c2da437"
      },
      "source": [
        "#Code #7\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(H, input_shape=(D,), activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(D))\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer =tf.keras.optimizers.SGD(lr=1e0))\n",
        "\n",
        "x = np.random.randn(N, D)\n",
        "y = np.random.randn(N, D)\n",
        "\n",
        "history = model.fit(x, y, epochs=50, batch_size=N)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 64 samples\n",
            "Epoch 1/50\n",
            "64/64 [==============================] - 0s 4ms/sample - loss: 1.1704\n",
            "Epoch 2/50\n",
            "64/64 [==============================] - 0s 97us/sample - loss: 1.1326\n",
            "Epoch 3/50\n",
            "64/64 [==============================] - 0s 86us/sample - loss: 1.1031\n",
            "Epoch 4/50\n",
            "64/64 [==============================] - 0s 83us/sample - loss: 1.0794\n",
            "Epoch 5/50\n",
            "64/64 [==============================] - 0s 81us/sample - loss: 1.0600\n",
            "Epoch 6/50\n",
            "64/64 [==============================] - 0s 93us/sample - loss: 1.0439\n",
            "Epoch 7/50\n",
            "64/64 [==============================] - 0s 86us/sample - loss: 1.0304\n",
            "Epoch 8/50\n",
            "64/64 [==============================] - 0s 89us/sample - loss: 1.0189\n",
            "Epoch 9/50\n",
            "64/64 [==============================] - 0s 92us/sample - loss: 1.0089\n",
            "Epoch 10/50\n",
            "64/64 [==============================] - 0s 92us/sample - loss: 1.0004\n",
            "Epoch 11/50\n",
            "64/64 [==============================] - 0s 97us/sample - loss: 0.9929\n",
            "Epoch 12/50\n",
            "64/64 [==============================] - 0s 103us/sample - loss: 0.9863\n",
            "Epoch 13/50\n",
            "64/64 [==============================] - 0s 75us/sample - loss: 0.9804\n",
            "Epoch 14/50\n",
            "64/64 [==============================] - 0s 79us/sample - loss: 0.9752\n",
            "Epoch 15/50\n",
            "64/64 [==============================] - 0s 94us/sample - loss: 0.9706\n",
            "Epoch 16/50\n",
            "64/64 [==============================] - 0s 106us/sample - loss: 0.9663\n",
            "Epoch 17/50\n",
            "64/64 [==============================] - 0s 126us/sample - loss: 0.9624\n",
            "Epoch 18/50\n",
            "64/64 [==============================] - 0s 148us/sample - loss: 0.9588\n",
            "Epoch 19/50\n",
            "64/64 [==============================] - 0s 87us/sample - loss: 0.9555\n",
            "Epoch 20/50\n",
            "64/64 [==============================] - 0s 104us/sample - loss: 0.9523\n",
            "Epoch 21/50\n",
            "64/64 [==============================] - 0s 100us/sample - loss: 0.9493\n",
            "Epoch 22/50\n",
            "64/64 [==============================] - 0s 98us/sample - loss: 0.9465\n",
            "Epoch 23/50\n",
            "64/64 [==============================] - 0s 71us/sample - loss: 0.9438\n",
            "Epoch 24/50\n",
            "64/64 [==============================] - 0s 66us/sample - loss: 0.9411\n",
            "Epoch 25/50\n",
            "64/64 [==============================] - 0s 96us/sample - loss: 0.9386\n",
            "Epoch 26/50\n",
            "64/64 [==============================] - 0s 82us/sample - loss: 0.9361\n",
            "Epoch 27/50\n",
            "64/64 [==============================] - 0s 96us/sample - loss: 0.9337\n",
            "Epoch 28/50\n",
            "64/64 [==============================] - 0s 76us/sample - loss: 0.9313\n",
            "Epoch 29/50\n",
            "64/64 [==============================] - 0s 111us/sample - loss: 0.9290\n",
            "Epoch 30/50\n",
            "64/64 [==============================] - 0s 114us/sample - loss: 0.9267\n",
            "Epoch 31/50\n",
            "64/64 [==============================] - 0s 140us/sample - loss: 0.9244\n",
            "Epoch 32/50\n",
            "64/64 [==============================] - 0s 100us/sample - loss: 0.9220\n",
            "Epoch 33/50\n",
            "64/64 [==============================] - 0s 95us/sample - loss: 0.9197\n",
            "Epoch 34/50\n",
            "64/64 [==============================] - 0s 85us/sample - loss: 0.9174\n",
            "Epoch 35/50\n",
            "64/64 [==============================] - 0s 89us/sample - loss: 0.9150\n",
            "Epoch 36/50\n",
            "64/64 [==============================] - 0s 98us/sample - loss: 0.9127\n",
            "Epoch 37/50\n",
            "64/64 [==============================] - 0s 92us/sample - loss: 0.9103\n",
            "Epoch 38/50\n",
            "64/64 [==============================] - 0s 93us/sample - loss: 0.9079\n",
            "Epoch 39/50\n",
            "64/64 [==============================] - 0s 89us/sample - loss: 0.9055\n",
            "Epoch 40/50\n",
            "64/64 [==============================] - 0s 92us/sample - loss: 0.9031\n",
            "Epoch 41/50\n",
            "64/64 [==============================] - 0s 97us/sample - loss: 0.9006\n",
            "Epoch 42/50\n",
            "64/64 [==============================] - 0s 87us/sample - loss: 0.8981\n",
            "Epoch 43/50\n",
            "64/64 [==============================] - 0s 88us/sample - loss: 0.8956\n",
            "Epoch 44/50\n",
            "64/64 [==============================] - 0s 99us/sample - loss: 0.8930\n",
            "Epoch 45/50\n",
            "64/64 [==============================] - 0s 95us/sample - loss: 0.8904\n",
            "Epoch 46/50\n",
            "64/64 [==============================] - 0s 90us/sample - loss: 0.8878\n",
            "Epoch 47/50\n",
            "64/64 [==============================] - 0s 95us/sample - loss: 0.8851\n",
            "Epoch 48/50\n",
            "64/64 [==============================] - 0s 121us/sample - loss: 0.8824\n",
            "Epoch 49/50\n",
            "64/64 [==============================] - 0s 124us/sample - loss: 0.8797\n",
            "Epoch 50/50\n",
            "64/64 [==============================] - 0s 235us/sample - loss: 0.8769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmbEEYWgpovX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}